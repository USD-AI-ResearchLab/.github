<!-- ### Advancing AI innovation with sustainability at its core -->
<!-- ### Powering AI innovation from the heart of the Rushmore State -->
<!-- ### Where Coyotes howl and AI innovation powers sustainable futures -->
<!-- ### South Dakota's AI powerhouse: Sustainable innovation carved from the Rushmore spirit -->
<!-- ### Howling at the frontier of sustainable AI: The Rushmore State's research powerhouse -->
<!-- ### The Coyote's AI powerhouse: Sustainable innovation from the Rushmore State -->

<div align="Left">
  <img src="https://github.com/USD-AI-ResearchLab/.github/raw/main/logo.png" alt="University of South Dakota AI Research Lab" width="350"/>
  
  <span style="display: inline; font-size: 25em; style= margin-top: -2000px;"> </strong>University of South Dakota AI Research Lab</strong></span>
  <span style="font-size: 1.2em; color: #718096; margin-left: 10px;">(formerly <a href="https://github.com/2ai-lab/" style="color: #4299e1;">2ai lab</a>)</span>
</div>

<div align="left" style="margin-top: 20px;">
  <p>
    <strong>Mission:</strong> <em>The Coyote's AI powerhouse on Sustainable innovation from the Rushmore State</em><br>
    <strong>Website:</strong> <a href="https://www.ai-research-lab.org/">ai-research-lab.org</a> • 
    <strong>Contact:</strong> <a href="usd.airesearch.lab@gmail.com">usd.airesearch.lab@gmail.com</a> • 
    <strong>Location:</strong> Vermillion, SD, USA
  </p>
</div>



We advance foundational AI and machine learning with a focus on sustainable, green computing to ensure efficiency and minimize carbon impact. Our interdisciplinary research spans computer vision, data mining, pattern recognition, and big data, impacting fields like healthcare, biometrics, forensics, speech, and IoT.
Join us as we drive AI innovation with sustainability at its core!



 Papers-with-Code 
-------


  <tr>
    <td width="50%">
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Non‑Uniform Illumination Attack for Fooling Convolutional Neural Networks</strong><br>
      <sub><em>NUI masks degrade CNNs; simple defense via NUI‑augmented training across CIFAR‑10, TinyImageNet, Caltech‑256.</em></sub><br>
      <a href="https://arxiv.org/abs/2409.03458"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="https://github.com/Akshayjain97/Non-Uniform_Illumination"><img alt="Code" src="https://img.shields.io/badge/Code-GitHub-0969DA?style=for-the-badge&logo=github"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
    <td width="50%">
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Advances and Challenges in Meta‑Learning: A Technical Review</strong><br>
      <sub><em>Survey across few‑shot, transfer, domain shift, self‑supervision, federated/personalized, continual; open problems.</em></sub><br>
      <a href="https://arxiv.org/pdf/2307.04722"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20Advances%20and%20Challenges%20in%20Meta-Learning"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
  </tr>

  <tr>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Advances in Deep Learning for Tuberculosis Screening using Chest X‑rays: The Last 5 Years Review</strong><br>
      <sub><em>Five‑year review of DL for TB CXR screening: datasets, methods, ROI localization, and challenges.</em></sub><br>
      <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9568934/"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20TB%20CXR%20Review"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Guest Editorial: Multimodal Learning in Medical Imaging Informatics</strong><br>
      <sub><em>Integrating heterogeneous clinical data (images, EHR, sensors, reports) for robust decision support.</em></sub><br>
      <a href="https://doi.org/10.1109/JBHI.2023.3241369"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20Multimodal%20Learning%20Editorial"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
  </tr>

  <tr>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Cervical cancerous cell classification: opposition‑based harmony search for deep feature selection</strong><br>
      <sub><em>CNN features + opposition‑based harmony search; strong results on Pap smear &amp; liquid‑based cytology.</em></sub><br>
      <a href="https://link.springer.com/article/10.1007/s13042-023-01872-z"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20Cervical%20O-bHSA"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>SecureFed: federated learning for lung abnormality analysis in chest X‑rays</strong><br>
      <sub><em>Secure aggregation for FL; robustness and fairness vs FedAvg/FedMGDA+/FedRAD on COVID‑19 CXRs.</em></sub><br>
      <a href="https://www.ai-research-lab.org/publication"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20SecureFed"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
  </tr>

  <tr>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>AI tools for assessing human fertility using risk factors: a state‑of‑the‑art review</strong><br>
      <sub><em>Systematic review of 42 studies; highlights augmentation, features, explainability for fertility‑risk analysis.</em></sub><br>
      <a href="https://link.springer.com/article/10.1007/s10916-023-01983-8"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20Fertility%20Risk%20Factors%20Review"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Hybrid approach for text categorization: Bangla news</strong><br>
      <sub><em>Hybrid text+graph features on 14,373 Bangla articles; Naïve Bayes Multinomial; validated on English sets.</em></sub><br>
      <a href="https://digitalcommons.isical.ac.in/journal-articles/3686/"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20Bangla%20Hybrid%20Text%20Categorization"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></img></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
  </tr>

  <tr>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Scale‑invariant preprocessing (ARES) to detect clusters of varying densities</strong><br>
      <sub><em>ARES rank transform enables KMeans/DBSCAN/Density‑Peak to recover varying‑density clusters more reliably.</em></sub><br>
      <a href="https://arxiv.org/abs/2401.11402"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20ARES%20Clustering%20Preprocessing"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Investigation of DNA discontinuity for detecting tuberculosis</strong><br>
      <sub><em>Automated pipeline to quantify DNA breaks using DNN + HMM on NCBI sequences; speed and accuracy gains.</em></sub><br>
      <a href="https://www.researchgate.net/publication/325272704_Investigation_of_DNA_Discontinuity_for_Detecting_Tuberculosis"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20DNA%20Discontinuity%20for%20TB"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
  </tr>

  <tr>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>LIFA: Language identification from audio with LPCC‑G features</strong><br>
      <sub><em>LPCC‑G + Random Forest across 11 Indian languages (&gt;2,200 hours); robust under noise.</em></sub><br>
      <a href="https://openreview.net/forum?id=n8sraRGFAT"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20LIFA%20(LPCC-G)"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Male fertility detection on skewed data with sampling + ensembles</strong><br>
      <sub><em>14 re‑sampling schemes; best LightGBM + SMOTE‑ENN; strong CatBoost baseline without re‑sampling.</em></sub><br>
      <a href="https://www.worldscientific.com/doi/abs/10.1142/S0218001424510033"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20Male%20Fertility%20Sampling%20%2B%20Ensembles"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
  </tr>

  <tr>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Shallow CNN for COVID‑19 outbreak screening using chest X‑rays</strong><br>
      <sub><em>Lightweight CNN achieves 99.69% accuracy and AUC 0.9995; very low false positives; 5‑fold CV.</em></sub><br>
      <a href="https://www.ai-research-lab.org/publication"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20Shallow%20CNN%20for%20COVID-19%20CXR"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>Covid‑19 Imaging Tools: How Big Data is Big?</strong><br>
      <sub><em>Dataset size, augmentation, transfer learning, and model‑fit caveats for COVID‑19 imaging tools.</em></sub><br>
      <a href="https://link.springer.com/article/10.1007/s10916-021-01747-2"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20COVID-19%20Imaging%20Tools%20(Big%20Data)"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
  </tr>

  <tr>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <strong>SegFast‑V2: Semantic image segmentation with fewer parameters</strong><br>
      <sub><em>Compact encoder‑decoder with kernel factorization &amp; depthwise deconvs; CPU‑friendly yet competitive.</em></sub><br>
      <a href="https://link.springer.com/article/10.1007/s13042-019-00906-2"><img alt="Paper" src="https://img.shields.io/badge/Paper-0969DA?style=for-the-badge"></a>
      <a href="mailto:contact@ai-research-lab.org?subject=Request%20code%3A%20COVID-19%20Imaging%20Tools%20(Big%20Data)"><img alt="Request code" src="https://img.shields.io/badge/Request%20code-Email-6e7781?style=for-the-badge&logo=gmail"></a>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
    <td>
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
      <!-- empty cell reserved for future items -->
      <p align="center"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="4"><rect width="100%" height="4" fill="#ffffff"/></svg></p>
    </td>
  </tr>

</div>
<!-- ------------------------------------------------------ -->

<!--### Research Overview
We design **reliable, transparent, and deployable AI** by pairing solid theory with field-tested engineering. Our work spans core ML and domain problems in health, environment, and mobility—always with **ethics** and **reproducibility** at the core.

--- ### Key Areas (pillars)
- **Trustworthy & Ethical AI** — interpretability, robustness, bias mitigation, rigorous evaluation.  
- **Representation & Multimodal Learning** — text, vision, geospatial, **hyperspectral**, time-series.  
- **ML Systems & Data Engineering** — scalable pipelines (batch/streaming), lakehouse, MLOps.  
- **Applied AI** — clinical early-warning, remote sensing for ecology/climate, urban mobility.

### Expertise (how we work)
- **Methods:** self-supervision, graph learning, causal inference, probabilistic modeling.  
- **Tooling:** Python/SQL, PyTorch/JAX, Spark/Databricks, Snowflake/Delta, Airflow/Prefect, Docker.  
- **Standards:** dataset cards, model cards, CI/CD for data & models, governance & documentation.

---

## Our People *(preview)*
**Purpose:** Build credibility and make connections easy.  
**Director:** \<Name, Title> • **Researchers & Students:** bios + GitHub/Scholar links • **Alumni:** placements & years • **Contributors:** org-wide credit via commits & PRs.

- Explore full directory → [`people.md`](./people.md)  
- Updated quarterly to reflect new members, graduations, and placements.

<p>
  <img alt=".github contributors" src="https://img.shields.io/github/contributors/USD-AI-ResearchLab/.github?label=.github%20contributors">
</p>

---

## Featured Projects *(preview)*
**Purpose:** Showcase active research and technical capabilities with concise, actionable links.

Each project uses a standard template (README, data card, model card, license, `CITATION.cff`) and includes a two-sentence pitch + GitHub link.

- **Hyperspectral Earth** — Robust embeddings for NEON/remote sensing to detect subtle ecological change.  
  *Method:* contrastive pretraining + domain adaptation; reproducible notebooks. *(See `projects.md`.)*

- **Urban Signals** — Real-time mobility forecasting with streaming pipelines (Kafka → Spark) and GNNs.  
  *Method:* feature stores + model monitoring for drift. *(See `projects.md`.)*

- **Clinical Early-Warning** — Interpretable time-series models with calibrated uncertainty.  
  *Method:* causal features + conformal prediction. *(See `projects.md`.)*

See the growing catalog → [`projects.md`](./projects.md)

---

## Publications & Conferences *(2015–2025, preview)*
**Purpose:** Demonstrate scholarly output across a decade without overwhelming the homepage.

**Highlights**
- **2015–2018** — Foundational theory & pilot deployments.  
- **2019–2021** — Reproducible pipelines, open datasets, scaling studies.  
- **2022–2025** — Multimodal systems, clinical/environmental deployments, policy-aware AI.

Full, citable list with external links → [`publications.md`](./publications.md)  
> Tip: From `publications.md`, link to Google Scholar or an institutional repository to reduce maintenance.

---

## Repository Archives *(preview)*
**Purpose:** Preserve institutional memory and research history.

Chronological index with short abstracts and artifact links (paper, dataset, code); each entry labeled **active**/**retired**.  
Browse the index → [`archives.md`](./archives.md)

---

## Join or Collaborate
**Purpose:** Convert interest into action via clear pathways.

### Opportunities
- **Students** — Undergraduate, Masters, PhD (include CV, links to code, and a 1-page research statement). Openings and expectations live in `people.md` → **Open Roles**.  
- **Postdocs** — Rolling consideration for candidates aligned with our pillars.  
- **Industry/Public Sector** — Sponsored research, dataset partnerships, technology transfer.

### How to Reach Us
- **Email:** collaborate@usd.edu *(replace with your inbox)*  
- **Interest Form:** https://github.com/USD-AI-ResearchLab *(until a dedicated page is live, open an Issue with label `collaboration`)*  
- **Office Hours:** Monthly virtual Q&A (see **Issues** with label `office-hours`).

> Proposals with open data/sharing plans and clear societal impact receive priority.

---

## Community & Conduct
**Purpose:** Establish professional standards and guidelines.

- **Code of Conduct:** We adopt the Contributor Covenant → [`CODE_OF_CONDUCT.md`](./CODE_OF_CONDUCT.md) *(add this file to the repo)*  
- **Collaboration Standards:** respectful communication, transparent authorship, conflict-of-interest disclosure, credit for contributors.  
- **Accessibility:** descriptive links, alt text for images, semantic headings, mobile-friendly rendering.  
- **Social:** Add links to X/Twitter, LinkedIn, Mastodon, YouTube in the lab website footer and `people.md`.

---
&nbsp;  
&nbsp;  
&nbsp;  

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

&nbsp;  
&nbsp;  
&nbsp;  

